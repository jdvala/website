<h1 id="sequence-classification-for-english-text">Sequence Classification for English Text</h1>
<h3 id="update">Update:</h3>
<p>Please go to the end of the post to see the updated results.</p>

<h3 id="aim">Aim:</h3>
<p>The aim of this model is to classify sentences. I am going to use Recurrent Neural Network for this purpose, specifically LSTMs. LSTMs have an advantage compared to simple RNN cell or GRU cell. You can learn about <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">LSTMs</a> and <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">RNNs</a>, There is no point on going in detail here. May be I will write another post wherein I will describe RNNs and LSTMs more intutively, but for today these two links will suffice.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import dependencies
</span><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>
<span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">imdb</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span>

<span class="c1"># Ignoring Gensim Warinings
</span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s">"ignore"</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let's load our model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'/home/jay/Saved_Models/english/english'</span><span class="p">)</span>
</code></pre></div></div>

<p>Firstly, we can not use this model directly into tensorflow. So what do we do now? Its simple we have to convert this model into numpy matrix that we can use to train our model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a matrix with the shape of Length of Vocab * total Dimensions 
# (for this model it will be 6442*4000, we have 6442 words in 4000 dimension)
</span>
<span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="mi">4000</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">)):</span>
    <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="k">if</span> <span class="n">embedding_vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">embedding_matrix</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(6442, 4000)
</code></pre></div></div>

<p>Thats about right, So we have loaded the model and created the embedding matrix to be used insted of embedding layer in Keras.</p>

<p>The next step would be to prepare the data to be feed into the model(LSTM), but we have prepared dataset for feeding to a neural network, so what is this now?? Well the dataset we created is not suitable for feeding it to neural network, there are a few steps that needs to be done before running the all fancy LSTM.</p>

<p>LSTMs take a fixed lenght sentences and also it will not take raw alpha numeric values, the words are to be converted into numbers, also all the sentences are to made with fixed lenght.</p>

<p>In the next step we are going to do that.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Lets load the dataset using pandas
</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'/home/jay/Data_Set_Creation/Data_to_Use.csv'</span><span class="p">)</span> 
<span class="n">dataset</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sentence</th>
      <th>Label</th>
      <th>Numerical_Label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>summary the directive seek ensure eu country f...</td>
      <td>transport</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>mlc set minimum global standard ensure right s...</td>
      <td>transport</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>pron also seek limit social dump secure fair c...</td>
      <td>transport</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>the main point new directive follow</td>
      <td>transport</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>monitor compliance eu country introduce effect...</td>
      <td>transport</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We can not use this dataframe so lets transform them to list
</span>
<span class="n">sent</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">Sentence</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">Numerical_Label</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</code></pre></div></div>

<p>For converting the words to numerical representations we have to tokenize the words</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Using Keras tokenizer
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">Tokenizer</span><span class="p">()</span>

<span class="c1"># Tokenizing the sentences (This process may take some time depending on your corpus size)
</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Lets see what our vocabulary size is
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>   <span class="c1"># We are adding 1 here because it takes indexing from zero
</span><span class="n">vocab_size</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>16024
</code></pre></div></div>

<p>We have 16,387 words in our vocabulary, Now let’s encode the sentences to its repecitve numbers, if you don’t get what I say right now be patitent, I will show an example which will clear it out.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sentence encoding
</span><span class="n">sent_encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>
</code></pre></div></div>

<p>So, back to what I was expalining how encoding works, Let’s say you have a document with only one sentence <strong>The quick brown fox jumps over the lazy dog</strong> and you want to use this sentence for training a RNN(LSTM), but the LSTM doesn’t understand this, it only understands numbers, so how to go on with this? Well its fairly simple, you tokenize the sentence and give each word a unique number.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Lets visualize whatever is written above
</span><span class="k">print</span><span class="p">(</span><span class="s">"{} </span><span class="se">\n</span><span class="s"> {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">sent</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">sent_encoded</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mlc set minimum global standard ensure right seafarer decent live working condition irrespective nationality irrespective flag ship serve : [7243, 42, 372, 364, 141, 31, 46, 2048, 2921, 870, 696, 99, 1674, 1577, 1674, 1218, 374, 971]
</code></pre></div></div>

<p>What the hell is that? I don’t understand. Well it means the word <em>mlc</em> is at index <em>7243</em> in the dictonary of our model. Don’t believe me? Let’s verify that.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">word_dict</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span>
<span class="n">word_dict</span><span class="p">[</span><span class="s">'mlc'</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>7243
</code></pre></div></div>

<p>Well well well, I should say I told you so. Now remember as we said that we need to make all the sentences in our dataset of a fixed length. This raises more questions, what is padding? how to do it?. Keras provides with functions to do it. So what is padding?</p>

<p>Padding is used to ensure that all sequences in a list have the same length, So by default <code class="highlighter-rouge">0</code> is added at the end. 
Example: We have a squence <code class="highlighter-rouge">[1, 2, 3], [3, 4, 5, 6], [7, 8]</code> which are all of different length. So if we pad it it will be <code class="highlighter-rouge">[1, 2, 3, 0], [3, 4, 5, 6], [7, 8, 0, 0]</code>, Note that we have added zeros at the end of the sentences, its called <em>post padding</em>, there is also <em>pre padding</em> where the zeros are added at the start of the squence.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># padding all the encoded sequences
# Before padding we need to define what should be the maximum padding length, for that we need to check what
# is the maximum length of sentence in out dataset, we can simply do it by
</span><span class="kn">import</span> <span class="nn">heapq</span>
<span class="n">heapq</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">sent</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="nb">len</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['lithuania adoption community acquisi lithuania adoption community acquisi lithuaniaarchif lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives',
 'slovakia adoption community acquisi slovakia adoption community acquisi slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives',
 'bulgaria adoption community acquisi bulgaria adoption community acquisi bulgariaarchives bulgariaarchives bulgariaarchives bulgariaarchives bulgariarchives bulgariaarchives bulgariaarchives bulgariaarchives bulgariaarchives bulgariaarchives bulgariaarchives bulgariaarchives',
 'hungary adoption community acquisi hungary adoption community acquisi hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives',
 'estonia adoption community acquis estonia adoption community acquis estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives']
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sorted</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span><span class="n">key</span><span class="o">=</span><span class="nb">len</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['aid amount set euro per box',
 'pron aim ensure pron safe use',
 'pron valid ten year may renew',
 'pron aim ensure pron safe use',
 'eu large aid donor global oda',
 'article set value upon eu base',
 'less red tape less risky trial',
 'datum may keep file three year',
 'one year term office may renew',
 'datum may keep file three year',
 'datum may keep file three year',
 'time sector may add pron scope',
 'datum may keep file three year',
 'article set value upon eu base',
 'date bank issue euro note coin',
 'community aid eur kg type milk',
 'diet calf feed least twice day',
 'word levy use cover tax charge',
  ...]
</code></pre></div></div>

<p>This looks promising, so I would like to go ahead with padding I described earlier. Keras provides simple padding function for us to use. Before we use that we have to define <code class="highlighter-rouge">max_lenght</code>, for padding we need to define the maximum size to which we pad the sequence. This is our <code class="highlighter-rouge">max_length</code>. And it is highly  advisable that we use the maximum length of sentence from our corpus.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Defining max_length
</span><span class="n">max_sent</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="nb">len</span><span class="p">)</span>    <span class="c1"># Get the longest sentence in the list
</span><span class="n">max_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">max_sent</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>  <span class="c1"># split it and set the max_length
</span><span class="n">max_length</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>20
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Start padding with the max_length
</span><span class="n">padded_sents</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">sequence</span><span class="o">.</span><span class="n">pad_sequences</span><span class="p">(</span><span class="n">sent_encoded</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'post'</span><span class="p">)</span>
<span class="c1"># Note: We are using post padding
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">padded_sents</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(128075, 20)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initializing weights matrix of the embedding layer
</span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">4000</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">embedding_matrix</span><span class="p">],</span> <span class="n">input_length</span><span class="o">=</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>NOTE</strong> that I have delibrately kept the <em>flag trainable=False</em>, because I have already trained the model once and I don’t want to train or change the weight matrix any further.</p>

<h3 id="spliting-the-dataset-into-test-and-train-sets">Spliting the dataset into test and train sets</h3>
<p>We have the whole dataset but for supervised training of a deep neural network we need training and testing set. 
Also in our dataset we have all the data stored squentially, meaning that all the classes are clustered which is not ideal, so firstly we will suffle the dataset then divide it into training and testing sets.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Split the data into test and train
</span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">train_set</span><span class="p">,</span> <span class="n">test_set</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span> <span class="n">test_label</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">padded_sents</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_set</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(85810, 20)
</code></pre></div></div>

<p><strong>NOTE:</strong> Keras expects numpy arrays, if you just provide it the training data in form of list, it will throw an error, so covert the list to arrays</p>

<p>Prepraing Labels is another task at hand, which needs to be done before feeding data into the neural network, agian using sklearn one hot encoding will do the trick</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>

<span class="c1"># Define one_hot_encoder object
</span><span class="n">onehot_encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">train_labels</span> <span class="o">=</span> <span class="n">onehot_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">train_label</span><span class="p">,(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">onehot_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">test_label</span><span class="p">,(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_labels</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(85810, 32)
</code></pre></div></div>

<h3 id="details-about-the-model">Details about the model</h3>
<p>Now that we have every thing ready we can start building the model, but there are a few questions that are needed to be answered before making the model, How many layers we are gonna use, what would be the activation function, how many neurons in each layer, what kind of regularization and what not. So I am going to build a very basic model and see how it does on the data. After the first results, I will be in a state to make changes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create sequential model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">embedding_layer</span><span class="p">)</span>   <span class="c1"># adding embedding layer, which we have defined earlier
</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span> <span class="c1"># LSTM layer with 50 units
</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span> <span class="c1"># LSTM layer with 50 units
</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">50</span><span class="p">))</span> <span class="c1"># LSTM layer with 50 units
</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 20, 4000)          64096000  
_________________________________________________________________
lstm_16 (LSTM)               (None, 20, 50)            810200    
_________________________________________________________________
lstm_17 (LSTM)               (None, 20, 50)            20200     
_________________________________________________________________
lstm_18 (LSTM)               (None, 50)                20200     
_________________________________________________________________
dense_6 (Dense)              (None, 32)                1632      
=================================================================
Total params: 64,948,232
Trainable params: 852,232
Non-trainable params: 64,096,000
_________________________________________________________________
None
Train on 77229 samples, validate on 8581 samples
Epoch 1/3
77229/77229 [==============================] - 309s 4ms/step - loss: 2.6437 - acc: 0.2746 - val_loss: 2.2583 - val_acc: 0.3832
Epoch 2/3
77229/77229 [==============================] - 344s 4ms/step - loss: 2.0753 - acc: 0.4300 - val_loss: 2.0260 - val_acc: 0.4301
Epoch 3/3
77229/77229 [==============================] - 392s 5ms/step - loss: 1.8524 - acc: 0.4794 - val_loss: 1.9480 - val_acc: 0.4477





&lt;keras.callbacks.History at 0x7ff3d8408128&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>42265/42265 [==============================] - 107s 3ms/step
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Accuracy: </span><span class="si">%.2</span><span class="s">f</span><span class="si">%%</span><span class="s">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy: 45.04%
</code></pre></div></div>

<p>The process was painstakingly slow, hence only performed 3 epochs, but we can conclued that there is still a chance of improvement if I have enough resources or a lot of patience(which I don’t have). So I will update this post with the updated results as soon as I have them. Till then, Have a great day.</p>

<h2 id="update-1">Update</h2>

<p>I managed to get a gpu enabled instance on google cloud. So I decided to run the algorithm there. The result is not that good but afterall its a trial which shed light on my dataset so I decided to update this post and pin in the results.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train on 77229 samples, validate on 8581 samples
2018-05-22 17:35:16.036372: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-05-22 17:35:16.513689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node <span class="nb">read </span>from SysFS had negative value <span class="o">(</span><span class="nt">-1</span><span class="o">)</span>, but there must be at least one NUMA node, so returning NUMA node zero
2018-05-22 17:35:16.514222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate<span class="o">(</span>GHz<span class="o">)</span>: 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-05-22 17:35:16.514250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-05-22 17:35:29.328739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-22 17:35:29.328808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-05-22 17:35:29.328818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-05-22 17:35:29.379302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device <span class="o">(</span>/job:localhost/replica:0/task:0/device:GPU:0 with 10747 MB memory<span class="o">)</span> -&gt; physical GPU <span class="o">(</span>device: 0, name: Tesla K80, pci bus <span class="nb">id</span>: 0000:00:04.0, compute capability: 3.7<span class="o">)</span>
Epoch 1/30
77229/77229 <span class="o">[==============================]</span> - 148s 2ms/step - loss: 2.4540 - acc: 0.3338 - val_loss: 2.0469 - val_acc: 0.4494
Epoch 2/30
77229/77229 <span class="o">[==============================]</span> - 126s 2ms/step - loss: 1.8124 - acc: 0.5052 - val_loss: 1.8633 - val_acc: 0.4860
Epoch 3/30
77229/77229 <span class="o">[==============================]</span> - 126s 2ms/step - loss: 1.5552 - acc: 0.5599 - val_loss: 1.8173 - val_acc: 0.4929
Epoch 4/30
77229/77229 <span class="o">[==============================]</span> - 127s 2ms/step - loss: 1.3846 - acc: 0.5952 - val_loss: 1.8096 - val_acc: 0.4868
Epoch 5/30
77229/77229 <span class="o">[==============================]</span> - 124s 2ms/step - loss: 1.2465 - acc: 0.6237 - val_loss: 1.8556 - val_acc: 0.4905
Epoch 6/30
77229/77229 <span class="o">[==============================]</span> - 126s 2ms/step - loss: 1.1322 - acc: 0.6489 - val_loss: 1.9120 - val_acc: 0.4833
Epoch 7/30
77229/77229 <span class="o">[==============================]</span> - 127s 2ms/step - loss: 1.0355 - acc: 0.6686 - val_loss: 1.9496 - val_acc: 0.4738
Epoch 8/30
77229/77229 <span class="o">[==============================]</span> - 128s 2ms/step - loss: 0.9489 - acc: 0.6863 - val_loss: 2.0237 - val_acc: 0.4705
Epoch 9/30
77229/77229 <span class="o">[==============================]</span> - 127s 2ms/step - loss: 0.8778 - acc: 0.7015 - val_loss: 2.1231 - val_acc: 0.4698
Epoch 10/30
77229/77229 <span class="o">[==============================]</span> - 128s 2ms/step - loss: 0.8160 - acc: 0.7145 - val_loss: 2.1606 - val_acc: 0.4650
Epoch 11/30
77229/77229 <span class="o">[==============================]</span> - 129s 2ms/step - loss: 0.7668 - acc: 0.7255 - val_loss: 2.2607 - val_acc: 0.4594
Epoch 12/30
77229/77229 <span class="o">[==============================]</span> - 127s 2ms/step - loss: 0.7277 - acc: 0.7324 - val_loss: 2.3309 - val_acc: 0.4572
Epoch 13/30
77229/77229 <span class="o">[==============================]</span> - 126s 2ms/step - loss: 0.6896 - acc: 0.7437 - val_loss: 2.4108 - val_acc: 0.4551
Epoch 14/30
77229/77229 <span class="o">[==============================]</span> - 125s 2ms/step - loss: 0.6581 - acc: 0.7490 - val_loss: 2.4212 - val_acc: 0.4541
Epoch 15/30
77229/77229 <span class="o">[==============================]</span> - 127s 2ms/step - loss: 0.6302 - acc: 0.7557 - val_loss: 2.4824 - val_acc: 0.4454
Epoch 16/30
77229/77229 <span class="o">[==============================]</span> - 128s 2ms/step - loss: 0.6087 - acc: 0.7588 - val_loss: 2.5772 - val_acc: 0.4463
Epoch 17/30
77229/77229 <span class="o">[==============================]</span> - 128s 2ms/step - loss: 0.5889 - acc: 0.7633 - val_loss: 2.6004 - val_acc: 0.4496
Epoch 18/30
77229/77229 <span class="o">[==============================]</span> - 128s 2ms/step - loss: 0.5739 - acc: 0.7672 - val_loss: 2.6675 - val_acc: 0.4369
Epoch 19/30
77229/77229 <span class="o">[==============================]</span> - 129s 2ms/step - loss: 0.5528 - acc: 0.7719 - val_loss: 2.7079 - val_acc: 0.4402
Epoch 20/30
77229/77229 <span class="o">[==============================]</span> - 127s 2ms/step - loss: 0.5413 - acc: 0.7748 - val_loss: 2.7494 - val_acc: 0.4378
Epoch 21/30
77229/77229 <span class="o">[==============================]</span> - 126s 2ms/step - loss: 0.5309 - acc: 0.7774 - val_loss: 2.8162 - val_acc: 0.4357
Epoch 22/30
77229/77229 <span class="o">[==============================]</span> - 125s 2ms/step - loss: 0.5209 - acc: 0.7780 - val_loss: 2.8276 - val_acc: 0.4360
Epoch 23/30
77229/77229 <span class="o">[==============================]</span> - 128s 2ms/step - loss: 0.5125 - acc: 0.7803 - val_loss: 2.8808 - val_acc: 0.4301
Epoch 24/30
77229/77229 <span class="o">[==============================]</span> - 128s 2ms/step - loss: 0.4969 - acc: 0.7844 - val_loss: 2.8847 - val_acc: 0.4360
Epoch 25/30
77229/77229 <span class="o">[==============================]</span> - 127s 2ms/step - loss: 0.4885 - acc: 0.7848 - val_loss: 2.9314 - val_acc: 0.4304
Epoch 26/30
77229/77229 <span class="o">[==============================]</span> - 128s 2ms/step - loss: 0.4869 - acc: 0.7844 - val_loss: 2.9672 - val_acc: 0.4328
Epoch 27/30
77229/77229 <span class="o">[==============================]</span> - 128s 2ms/step - loss: 0.4778 - acc: 0.7890 - val_loss: 3.0229 - val_acc: 0.4336
Epoch 28/30
77229/77229 <span class="o">[==============================]</span> - 128s 2ms/step - loss: 0.4750 - acc: 0.7870 - val_loss: 3.0632 - val_acc: 0.4298
Epoch 29/30
77229/77229 <span class="o">[==============================]</span> - 128s 2ms/step - loss: 0.4697 - acc: 0.7894 - val_loss: 3.0448 - val_acc: 0.4314
Epoch 30/30
77229/77229 <span class="o">[==============================]</span> - 128s 2ms/step - loss: 0.4620 - acc: 0.7892 - val_loss: 3.0588 - val_acc: 0.4236
42265/42265 <span class="o">[==============================]</span> - 37s 869us/step
Accuracy: 42.71%
             precision    recall  f1-score   support

          0       0.58      0.57      0.58      2703
          1       0.26      0.31      0.28       438
          2       0.36      0.41      0.38      1530
          3       0.36      0.38      0.37      3955
          4       0.62      0.60      0.61      4794
          5       0.36      0.34      0.35      1536
          6       0.69      0.58      0.63      1712
          7       0.34      0.22      0.27       498
          8       0.29      0.33      0.31      1102
          9       0.25      0.25      0.25       829
         10       0.53      0.53      0.53      1118
         11       0.24      0.27      0.26      1126
         12       0.34      0.31      0.32       679
         13       0.39      0.36      0.37       285
         14       0.31      0.30      0.30       822
         15       0.40      0.43      0.41      2865
         16       0.31      0.31      0.31      1326
         17       0.26      0.21      0.23       506
         18       0.29      0.31      0.30       413
         19       0.23      0.18      0.20       829
         20       0.41      0.39      0.40      1198
         21       0.47      0.45      0.46      1345
         22       0.28      0.26      0.27       409
         23       0.26      0.26      0.26       344
         24       0.47      0.54      0.50      3541
         25       0.42      0.37      0.40      1271
         26       0.57      0.60      0.59      1549
         27       0.18      0.14      0.16       600
         28       0.51      0.47      0.49       732
         29       0.31      0.31      0.31       307
         30       0.39      0.34      0.37       639
         31       0.30      0.30      0.30      1264

avg / total       0.43      0.43      0.43     42265

<span class="o">[[</span>1547    3   27 ...    2    4   23]
 <span class="o">[</span>   4  135   13 ...    0    0   54]
 <span class="o">[</span>  27   21  631 ...    3    2   32]
 ...
 <span class="o">[</span>   4    2    5 ...   94    3    1]
 <span class="o">[</span>   3    7    1 ...    2  220   19]
 <span class="o">[</span>  28   62   35 ...    0   19  374]]

</code></pre></div></div>
